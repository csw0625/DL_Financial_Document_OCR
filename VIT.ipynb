{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3384e323",
   "metadata": {},
   "source": [
    "# Vision Transformer (ViT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c53e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libarary import and setting\n",
    "import os, csv, json, unicodedata\n",
    "import math, random, re\n",
    "from typing import Optional, List\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import functional as TF\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "# device, seed 설정\n",
    "device = torch.device(\n",
    "    \"mps\" if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "SEED = 42\n",
    "random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "# 경로 설정\n",
    "CSV_TRAIN = \"../text_crops/train_labels.csv\"\n",
    "CSV_VAL = \"../text_crops/valid_labels.csv\"\n",
    "CHARSET_JSON = \"charset.json\"\n",
    "save_dir  = \"../vitctc_best.pt\"\n",
    "\n",
    "# ViT 입력 설정\n",
    "IMG_H = 64      # 이미지 세로 길이 고정\n",
    "IN_CH = 1       # 입력 채널 수 (흑백=1)\n",
    "PATCH_W = 8     # 패치 너비\n",
    "STRIDE = 8      # 패치 스트라이드\n",
    "\n",
    "# 학습 설정\n",
    "BATCH = 32\n",
    "LR = 0.0003      \n",
    "EPOCHS = 10\n",
    "\n",
    "# 모델 크기\n",
    "D_MODEL = 256   #hidden dimension\n",
    "N_HEAD = 8      #attention head 수\n",
    "N_LAYERS = 4    #encoder layer 수\n",
    "FF_DIM = 1024   #feed-forward hidden dimension\n",
    "\n",
    "# charset class 설정\n",
    "class Charset:\n",
    "    def __init__(self, itos, blank_idx=0):\n",
    "        self.itos = itos\n",
    "        self.blank_idx = blank_idx\n",
    "        self.stoi = {ch:i for i,ch in enumerate(itos)}\n",
    "\n",
    "    def encode(self, s):\n",
    "    # 문자열 -> 인덱스 리스트\n",
    "        return [self.stoi[ch] for ch in s if ch in self.stoi]\n",
    "\n",
    "    def decode_ctc(self, seq: List[int]) -> str:\n",
    "    # CTC 결과 -> 문자열\n",
    "        res, prev = [], None\n",
    "        for s in seq:\n",
    "            if s != self.blank_idx and s != prev:\n",
    "                res.append(self.itos[s])\n",
    "            prev = s\n",
    "        return \"\".join(res)\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "with open(\"charset.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    itos = json.load(f)\n",
    "charset = Charset(itos, blank_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c633b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 데이터 관련 함수\n",
    "# data read 함수\n",
    "DATA_ROOT = \"../text_crops\"\n",
    "\n",
    "def resolve_path(p: str) -> str:\n",
    "    p = p[32:]\n",
    "    p = p.strip().replace(\"\\\\\", \"/\")\n",
    "    return os.path.normpath(os.path.join(DATA_ROOT, p))\n",
    "\n",
    "def read_rows(csv_path, path_key=\"crop_path\", text_key=\"text\"):\n",
    "    rows = []\n",
    "    with open(csv_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        r = csv.DictReader(f)\n",
    "        for row in r:\n",
    "            p = resolve_path(row.get(path_key))\n",
    "            t = row.get(text_key)\n",
    "            rows.append({\"crop_path\": p, \"text\": t})\n",
    "    print(f\"[{csv_path}] loaded {len(rows)} rows\")\n",
    "    return rows\n",
    "\n",
    "# 이미지 전처리 함수\n",
    "class Resize:\n",
    "    \"\"\"이미지 세로길이 통일하며 원본 비율 유지\"\"\"\n",
    "    def __init__(self, h: int):\n",
    "        self.h = h\n",
    "\n",
    "    def __call__(self, img: Image.Image) -> Image.Image:\n",
    "        w, h = img.size\n",
    "        new_w = max(1, int(w * (self.h / float(h))))\n",
    "        return img.resize((new_w, self.h), Image.BILINEAR)\n",
    "\n",
    "def resize_keep_ratio_pad(img: Image.Image, target_h: int, target_min_w: int, pad_multiple: int):\n",
    "    \"\"\"ViT용 이미지 전처리: 비율 유지 + 패딩\"\"\"\n",
    "    w, h = img.size\n",
    "    scale = target_h / h\n",
    "    new_w = max(target_min_w, int(math.ceil(w * scale)))\n",
    "    img = img.resize((new_w, target_h), Image.BILINEAR)\n",
    "    \n",
    "    pad_w = int(math.ceil(new_w / pad_multiple) * pad_multiple)\n",
    "    canvas = Image.new('L', (pad_w, target_h), color=255)\n",
    "    canvas.paste(img, (0, 0))\n",
    "    return canvas, new_w, pad_w\n",
    "\n",
    "class OCRDataset(Dataset):\n",
    "    \"\"\"CSV(crop_path,text) → (tensor[C,H,W], target[int-seq])\"\"\"\n",
    "    def __init__(self, csv_path: str, charset: Charset, target_h: int = IMG_H, path_key=\"crop_path\"):\n",
    "        self.rows = read_rows(csv_path, path_key=path_key, text_key=\"text\")\n",
    "        self.cs = charset\n",
    "        self.norm = \"NFC\"\n",
    "        self.target_h = target_h\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rows)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        r = self.rows[i]\n",
    "        \n",
    "        img = Image.open(r[\"crop_path\"]).convert(\"L\")  # 흑백\n",
    "        # ViT용 전처리 적용\n",
    "        img_resized, new_w, pad_w = resize_keep_ratio_pad(\n",
    "            img, self.target_h, PATCH_W, PATCH_W\n",
    "        )\n",
    "        x = TF.to_tensor(img_resized)  # [1,H,W]\n",
    "        \n",
    "        txt = unicodedata.normalize(self.norm, r[\"text\"])\n",
    "        txt = \"\".join(ch for ch in txt if ch in self.cs.stoi)  # OOV 제거\n",
    "        y = torch.tensor(self.cs.encode(txt), dtype=torch.long)\n",
    "        \n",
    "        # ViT용 추가 정보\n",
    "        steps = int(math.ceil(new_w / PATCH_W))\n",
    "        \n",
    "        return x, y, steps\n",
    "\n",
    "def vit_collate(batch):\n",
    "    \"\"\"ViT용 배치 collate 함수\"\"\"\n",
    "    xs, ys, steps_list = zip(*batch)\n",
    "    \n",
    "    # 이미지 패딩 (배치 내 최대 너비로 통일)\n",
    "    C, H = xs[0].shape[:2]\n",
    "    max_W = max(x.shape[2] for x in xs)\n",
    "    xs_pad = [F.pad(x, (0, max_W - x.shape[2], 0, 0)) for x in xs]\n",
    "    xs = torch.stack(xs_pad, 0)  # [B,C,H,max_W]\n",
    "    \n",
    "    # CTC용 타겟 준비\n",
    "    y_lens = torch.tensor([y.numel() for y in ys], dtype=torch.long)\n",
    "    ys = torch.cat(ys, 0)  # [sum(L)]\n",
    "    \n",
    "    # ViT용 입력 길이\n",
    "    input_lengths = torch.tensor(steps_list, dtype=torch.long)\n",
    "    \n",
    "    return xs, ys, y_lens, input_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e86fc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ViT 모델 (위치 인코딩 포함)\n",
    "\n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    \"\"\"사인/코사인 위치 인코딩\"\"\"\n",
    "    def __init__(self, d_model: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        T = x.size(0)\n",
    "        device = x.device\n",
    "        pe = torch.zeros(T, self.d_model, device=device)\n",
    "        pos = torch.arange(0, T, device=device).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, self.d_model, 2, device=device) * \n",
    "                       (-math.log(10000.0) / self.d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        return pe\n",
    "\n",
    "class ViTCTC(nn.Module):\n",
    "    \"\"\"ViT 모델\"\"\"\n",
    "    def __init__(self, vocab_size: int, img_h=IMG_H, patch_w=PATCH_W, \n",
    "                 d_model=D_MODEL, nhead=N_HEAD, num_layers=N_LAYERS, dim_ff=FF_DIM):\n",
    "        super().__init__()\n",
    "        self.img_h = img_h\n",
    "        self.patch_w = patch_w\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # 패치 임베딩 (이미지를 세로 스트립으로 분할)\n",
    "        self.patch_embed = nn.Conv2d(1, d_model, kernel_size=(img_h, patch_w), \n",
    "                                   stride=(img_h, patch_w))\n",
    "        \n",
    "        # 트랜스포머 인코더\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_ff,\n",
    "            dropout=0.1, activation='relu', batch_first=False, norm_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "        \n",
    "        # 위치 인코딩\n",
    "        self.pos_enc = SinusoidalPositionalEncoding(d_model)\n",
    "        \n",
    "        # 분류 헤드\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 패치 임베딩: [B,C,H,W] → [T,B,D]\n",
    "        tokens = self.patch_embed(x).squeeze(2).permute(2, 0, 1)  # [T,B,D]\n",
    "        \n",
    "        # 위치 인코딩 추가\n",
    "        tokens = tokens + self.pos_enc(tokens).unsqueeze(1)\n",
    "        \n",
    "        # 트랜스포머 인코딩\n",
    "        enc = self.encoder(tokens)  # [T,B,D]\n",
    "        \n",
    "        # 분류\n",
    "        logits = self.fc(enc).permute(1,0,2)  # [T,B,V]\n",
    "        \n",
    "        return logits, tokens.size(0)  # logits, sequence_length\n",
    "    \n",
    "# train/val dataset & dataloader\n",
    "train_set = OCRDataset(CSV_TRAIN, charset, target_h=IMG_H, path_key=\"crop_path\")\n",
    "val_set = OCRDataset(CSV_VAL, charset, target_h=IMG_H, path_key=\"crop_path\")\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH, shuffle=True, collate_fn=vit_collate)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH, shuffle=False, collate_fn=vit_collate)\n",
    "\n",
    "# model\n",
    "VOCAB_SIZE = charset.size\n",
    "model = ViTCTC(vocab_size=VOCAB_SIZE, img_h=IMG_H, patch_w=PATCH_W,\n",
    "               d_model=D_MODEL, nhead=N_HEAD, num_layers=N_LAYERS, \n",
    "               dim_ff=FF_DIM).to(device)\n",
    "\n",
    "criterion = nn.CTCLoss(blank=charset.blank_idx, zero_infinity=True)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791430a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ctc header functions\n",
    "def ctc_loss_fn(model_output, targets, input_lengths, target_lengths, blank=0):\n",
    "    \"\"\"CTC loss 계산\"\"\"\n",
    "    if isinstance(model_output, tuple):\n",
    "        logits = model_output[0]  \n",
    "    else:\n",
    "        logits = model_output\n",
    "    \n",
    "    log_probs = F.log_softmax(logits.permute(1,0,2), dim=-1).cpu()    # (T,B,C)\n",
    "    return F.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank=blank, zero_infinity=True)\n",
    "\n",
    "@torch.no_grad()\n",
    "def greedy_decode(model_output, input_lengths, blank=0):\n",
    "    \"\"\"CTC greedy 디코딩\"\"\"\n",
    "    if isinstance(model_output, tuple):\n",
    "        logits = model_output[0]  \n",
    "    else:\n",
    "        logits = model_output\n",
    "        \n",
    "    preds = logits.argmax(-1)  # (B,T)\n",
    "    out = []\n",
    "    for b in range(preds.size(0)):\n",
    "        L = int(input_lengths[b].item())\n",
    "        out.append(preds[b,:L].tolist())\n",
    "    return out\n",
    "\n",
    "def _levenshtein(a: str, b: str) -> int:\n",
    "    \"\"\"레벤슈타인 거리 계산 (공간 최적화 버전)\"\"\"\n",
    "    n, m = len(a), len(b)\n",
    "    dp = list(range(m+1))\n",
    "    for i in range(1, n+1):\n",
    "        prev = dp[0]; dp[0] = i\n",
    "        for j in range(1, m+1):\n",
    "            tmp = dp[j]\n",
    "            cost = 0 if a[i-1]==b[j-1] else 1\n",
    "            dp[j] = min(dp[j]+1, dp[j-1]+1, prev+cost)\n",
    "            prev = tmp\n",
    "    return dp[m]\n",
    "\n",
    "def cer(ref: str, hyp: str) -> float:\n",
    "    \"\"\"문자 단위 CER 계산\"\"\"\n",
    "    if len(ref)==0:\n",
    "        return 0.0 if len(hyp)==0 else 1.0\n",
    "    return _levenshtein(ref, hyp) / len(ref)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, charset, device):\n",
    "    \"\"\"모델 평가 함수\"\"\"\n",
    "    model.eval()\n",
    "    total = 0.0\n",
    "    n = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        # 튜플 언패킹 (vit_collate 함수에 맞춤)\n",
    "        imgs, targets, target_lengths, input_lengths = batch\n",
    "        imgs = imgs.to(device)\n",
    "        input_lengths = input_lengths.to(device)\n",
    "        \n",
    "        model_output = model(imgs)\n",
    "        pred_ids = greedy_decode(model_output, input_lengths, blank=charset.blank_idx)\n",
    "        preds = [charset.decode_ctc(s) for s in pred_ids]\n",
    "        \n",
    "        # CER 계산\n",
    "        target_start = 0\n",
    "        for i, p in enumerate(preds):\n",
    "            target_length = int(target_lengths[i].item())\n",
    "            target_end = target_start + target_length\n",
    "            target_ids = targets[target_start:target_end].tolist()\n",
    "            r = charset.decode_ctc(target_ids)\n",
    "            \n",
    "            total += cer(r, p)\n",
    "            n += 1\n",
    "            target_start = target_end\n",
    "    \n",
    "    return total / max(1, n)\n",
    "\n",
    "def train(model, train_loader, val_loader, charset, device, epochs=EPOCHS, lr=LR):\n",
    "    \"\"\"모델 학습 함수\"\"\"\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    best = 1e9\n",
    "    \n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total = 0.0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            imgs, targets, target_lengths, input_lengths = batch\n",
    "            \n",
    "            imgs = imgs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            input_lengths = input_lengths.to(device)\n",
    "            target_lengths = target_lengths.to(device)\n",
    "            \n",
    "            model_output = model(imgs)\n",
    "            loss = ctc_loss_fn(model_output, targets, input_lengths, target_lengths, blank=charset.blank_idx)\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            opt.step()\n",
    "            \n",
    "            total += loss.item()\n",
    "        \n",
    "        val_cer = evaluate(model, val_loader, charset, device)\n",
    "        print(f\"[{ep}] train_loss={total/len(train_loader):.4f} | val_CER={val_cer:.4f}\")\n",
    "        \n",
    "        if val_cer < best:\n",
    "            best = val_cer\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            torch.save({\"model\": model.state_dict(),\n",
    "                       \"config\": {\"img_h\": model.img_h, \"patch_w\": model.patch_w,\n",
    "                                  \"d_model\": model.d_model, \"num_classes\": charset.size}},\n",
    "                       f\"{save_dir}/best_vit_ctc.pt\")\n",
    "            \n",
    "            with open(f\"{save_dir}/charset.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "                json.dump(charset.itos[1:], f, ensure_ascii=False, indent=2)\n",
    "            print(\"  saved:\", f\"{save_dir}/best_vit_ctc0909.pt\")\n",
    "    return best\n",
    "\n",
    "# Vit-CTC 모델 학습 결과 최종 출력\n",
    "best_cer = train(model, train_loader, val_loader, charset, device)\n",
    "print(\"최종 best CER:\", best_cer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
