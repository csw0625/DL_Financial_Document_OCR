{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3ec0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train samples: 265413, valid samples: 17145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 01: 100%|██████████████████████████████████| 531/531 [05:06<00:00,  1.73it/s, ips=866.7, loss=5.0740, lr=1.00e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 01] train_loss=5.0740  valid_CER=94.57%\n",
      "  >> Saved BEST to crnn_ctc_best3.pth (CER 94.57%)\n",
      "  ex1: REF='474387-05-714500' | HYP='2'\n",
      "  ex2: REF='탕시우' | HYP='2'\n",
      "  ex3: REF='부산광역시' | HYP='2'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 02: 100%|██████████████████████████████████| 531/531 [05:03<00:00,  1.75it/s, ips=875.3, loss=3.0941, lr=1.00e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 02] train_loss=3.0941  valid_CER=25.86%\n",
      "  >> Saved BEST to crnn_ctc_best3.pth (CER 25.86%)\n",
      "  ex1: REF='474387-05-714500' | HYP='474387-05-714500'\n",
      "  ex2: REF='탕시우' | HYP='명서우'\n",
      "  ex3: REF='부산광역시' | HYP='부산광역시'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 03: 100%|██████████████████████████████████| 531/531 [04:59<00:00,  1.77it/s, ips=884.8, loss=0.6370, lr=1.00e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 03] train_loss=0.6370  valid_CER=9.16%\n",
      "  >> Saved BEST to crnn_ctc_best3.pth (CER 9.16%)\n",
      "  ex1: REF='474387-05-714500' | HYP='474387-05-714500'\n",
      "  ex2: REF='탕시우' | HYP='탕서우'\n",
      "  ex3: REF='부산광역시' | HYP='부산광역시'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 04: 100%|██████████████████████████████████| 531/531 [04:53<00:00,  1.81it/s, ips=903.6, loss=0.2722, lr=1.00e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 04] train_loss=0.2722  valid_CER=5.45%\n",
      "  >> Saved BEST to crnn_ctc_best3.pth (CER 5.45%)\n",
      "  ex1: REF='474387-05-714500' | HYP='474387-05-714500'\n",
      "  ex2: REF='탕시우' | HYP='담시우'\n",
      "  ex3: REF='부산광역시' | HYP='부산광역시'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 05: 100%|██████████████████████████████████| 531/531 [04:55<00:00,  1.80it/s, ips=897.5, loss=0.1668, lr=1.00e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 05] train_loss=0.1668  valid_CER=3.98%\n",
      "  >> Saved BEST to crnn_ctc_best3.pth (CER 3.98%)\n",
      "  ex1: REF='474387-05-714500' | HYP='474387-05-714500'\n",
      "  ex2: REF='탕시우' | HYP='탕시우'\n",
      "  ex3: REF='부산광역시' | HYP='부산광역시'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 06: 100%|██████████████████████████████████| 531/531 [04:57<00:00,  1.79it/s, ips=893.0, loss=0.1165, lr=1.00e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 06] train_loss=0.1165  valid_CER=3.10%\n",
      "  >> Saved BEST to crnn_ctc_best3.pth (CER 3.10%)\n",
      "  ex1: REF='474387-05-714500' | HYP='474387-05-714500'\n",
      "  ex2: REF='탕시우' | HYP='탕시우'\n",
      "  ex3: REF='부산광역시' | HYP='부산광역시'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 07: 100%|██████████████████████████████████| 531/531 [04:59<00:00,  1.78it/s, ips=887.4, loss=0.0857, lr=1.00e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 07] train_loss=0.0857  valid_CER=2.60%\n",
      "  >> Saved BEST to crnn_ctc_best3.pth (CER 2.60%)\n",
      "  ex1: REF='474387-05-714500' | HYP='474387-05-714500'\n",
      "  ex2: REF='탕시우' | HYP='탕시우'\n",
      "  ex3: REF='부산광역시' | HYP='부산광역시'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 08: 100%|██████████████████████████████████| 531/531 [04:52<00:00,  1.82it/s, ips=907.2, loss=0.0661, lr=1.00e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 08] train_loss=0.0661  valid_CER=2.17%\n",
      "  >> Saved BEST to crnn_ctc_best3.pth (CER 2.17%)\n",
      "  ex1: REF='474387-05-714500' | HYP='474387-05-714500'\n",
      "  ex2: REF='탕시우' | HYP='탕시우'\n",
      "  ex3: REF='부산광역시' | HYP='부산광역시'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 09: 100%|██████████████████████████████████| 531/531 [05:05<00:00,  1.74it/s, ips=868.8, loss=0.0507, lr=1.00e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 09] train_loss=0.0507  valid_CER=2.16%\n",
      "  >> Saved BEST to crnn_ctc_best3.pth (CER 2.16%)\n",
      "  ex1: REF='474387-05-714500' | HYP='474387-05-714500'\n",
      "  ex2: REF='탕시우' | HYP='탕시우'\n",
      "  ex3: REF='부산광역시' | HYP='부산광역시'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████████████████████████████| 531/531 [04:59<00:00,  1.77it/s, ips=884.9, loss=0.0413, lr=1.00e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] train_loss=0.0413  valid_CER=1.78%\n",
      "  >> Saved BEST to crnn_ctc_best3.pth (CER 1.78%)\n",
      "  ex1: REF='474387-05-714500' | HYP='474387-05-714500'\n",
      "  ex2: REF='탕시우' | HYP='탕시우'\n",
      "  ex3: REF='부산광역시' | HYP='부산광역시'\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os, csv, math, unicodedata, random\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ===================== 경로/하이퍼파라미터 =====================\n",
    "ROOT = r\"C:\\Users\\USER\\DL_OCR\\dataset\\crnn_crops\"\n",
    "TRAIN_CSV = Path(ROOT) / \"train_labels.csv\"\n",
    "VALID_CSV = Path(ROOT) / \"valid_labels.csv\"\n",
    "\n",
    "TRAIN_VARIANTS = {\"gt_pad_blur\", \"det_pad\"}\n",
    "VALID_VARIANTS = {\"gt_pad\"}\n",
    "\n",
    "IMG_HEIGHT = 32\n",
    "MAX_WIDTH  = 512            \n",
    "BATCH_SIZE = 500            \n",
    "EPOCHS = 10                 \n",
    "LR = 1e-3                   \n",
    "WEIGHT_DECAY = 0.0\n",
    "EARLY_STOP_PATIENCE = 5     \n",
    "NUM_WORKERS = 0            \n",
    "SEED = 1337\n",
    "SAVE_PATH = \"crnn_ctc_best3.pth\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "# ===================== 유틸 =====================\n",
    "def normalize_text(s: str) -> str:\n",
    "    if s is None: return \"\"\n",
    "    s = s.strip(\"\\n\\r\\t \")\n",
    "    return unicodedata.normalize(\"NFKC\", s)\n",
    "\n",
    "def read_csv_filter(csv_path: Path, variants: set) -> List[Tuple[str,str]]:\n",
    "    rows = []\n",
    "    with open(csv_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        r = csv.DictReader(f)\n",
    "        for row in r:\n",
    "            if variants and row[\"variant\"] not in variants:\n",
    "                continue\n",
    "            img_path = row[\"img_path\"]\n",
    "            text = normalize_text(row[\"text\"])\n",
    "            if len(text) == 0:\n",
    "                continue\n",
    "            rows.append((img_path, text))\n",
    "    return rows\n",
    "\n",
    "def build_vocab(samples: List[Tuple[str,str]]) -> Tuple[Dict[str,int], List[str]]:\n",
    "    charset = set()\n",
    "    for _, txt in samples:\n",
    "        charset.update(list(txt))\n",
    "    itos = sorted(list(charset))\n",
    "    stoi = {ch:i for i,ch in enumerate(itos)}\n",
    "    return stoi, itos\n",
    "\n",
    "def text_to_indices(text: str, stoi: Dict[str,int]) -> List[int]:\n",
    "    return [stoi[ch] for ch in text if ch in stoi]\n",
    "\n",
    "# ===================== 전처리/데이터셋 =====================\n",
    "class CRNNDataset(Dataset):\n",
    "    def __init__(self, rows: List[Tuple[str,str]], stoi: Dict[str,int],\n",
    "                 img_h: int = IMG_HEIGHT, max_w: int = MAX_WIDTH):\n",
    "        self.rows = rows\n",
    "        self.stoi = stoi\n",
    "        self.img_h = img_h\n",
    "        self.max_w = max_w\n",
    "\n",
    "    def __len__(self): return len(self.rows)\n",
    "\n",
    "    def _resize_keep_ratio_pad(self, img: Image.Image):\n",
    "        # 높이 고정, 가로 비율 유지 → 우측 패딩\n",
    "        w, h = img.size\n",
    "        scale = self.img_h / h\n",
    "        new_w = max(1, int(round(w * scale)))\n",
    "        new_w = min(new_w, self.max_w)\n",
    "        img = img.resize((new_w, self.img_h), Image.BILINEAR)\n",
    "        canvas = Image.new(\"L\", (self.max_w, self.img_h), 255)\n",
    "        canvas.paste(img, (0, 0))\n",
    "        return np.array(canvas), new_w\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, text = self.rows[idx]\n",
    "        img = Image.open(img_path).convert(\"L\")\n",
    "        img_np, valid_w = self._resize_keep_ratio_pad(img)\n",
    "        img_t = torch.from_numpy(img_np).unsqueeze(0).float() / 255.0\n",
    "        target_idx = torch.tensor(text_to_indices(text, self.stoi), dtype=torch.long)\n",
    "        return img_t, valid_w, target_idx, img_path\n",
    "\n",
    "def crnn_collate(batch, blank_idx: int):\n",
    "    imgs, widths, targets, paths = zip(*batch)\n",
    "    imgs = torch.stack(imgs, dim=0)  # [B,1,H,Wmax]\n",
    "    # 다운샘플: MaxPool2d 두 번 → 가로축 stride=4\n",
    "    input_lengths = torch.tensor([math.ceil(w/4) for w in widths], dtype=torch.long)\n",
    "    target_concat = torch.cat(targets)\n",
    "    target_lengths = torch.tensor([len(t) for t in targets], dtype=torch.long)\n",
    "    return imgs, input_lengths, target_concat, target_lengths, paths\n",
    "\n",
    "# ===================== CRNN 모델 =====================\n",
    "class CRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN(3층) → H-dim mean pooling → BiLSTM(2층, hidden=256) → Linear\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes: int, img_h: int = IMG_HEIGHT, cnn_out: int = 256,\n",
    "                 hidden: int = 256, layers: int = 2):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1,  64, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d(2,2),  # stride 2\n",
    "            nn.Conv2d(64,128, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d(2,2),  # stride 4\n",
    "            nn.Conv2d(128, cnn_out, 3, 1, 1), nn.ReLU(True),\n",
    "        )\n",
    "        # RNN 입력차원은 cnn_out (H축 평균을 취하므로)\n",
    "        self.rnn = nn.LSTM(input_size=cnn_out, hidden_size=hidden,\n",
    "                           num_layers=layers, batch_first=False, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden*2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B,1,32,W]\n",
    "        f = self.cnn(x)        # [B,C=256,H',W'], H' = 32/4 = 8\n",
    "        f = f.mean(2)          # H' 평균 → [B,C,W']\n",
    "        f = f.permute(2,0,1)   # [W',B,C]  (T,B,D)\n",
    "        y,_ = self.rnn(f)      # [T,B,2H]\n",
    "        return self.fc(y)      # [T,B,C(num_classes)]\n",
    "\n",
    "# ===================== 디코딩/지표 =====================\n",
    "def greedy_decode(logits: torch.Tensor, input_lengths: torch.Tensor, blank_idx: int) -> List[List[int]]:\n",
    "    probs = logits.log_softmax(dim=-1)\n",
    "    pred = probs.argmax(dim=-1)  # [T,B]\n",
    "    T, B = pred.shape\n",
    "    hyps = []\n",
    "    for b in range(B):\n",
    "        seq, prev = [], -1\n",
    "        Tvalid = min(int(input_lengths[b].item()), T)\n",
    "        for t in range(Tvalid):\n",
    "            p = int(pred[t, b].item())\n",
    "            if p != blank_idx and p != prev:\n",
    "                seq.append(p)\n",
    "            prev = p\n",
    "        hyps.append(seq)\n",
    "    return hyps\n",
    "\n",
    "def cer(ref: str, hyp: str) -> float:\n",
    "    n, m = len(ref), len(hyp)\n",
    "    dp = np.zeros((n+1, m+1), dtype=np.int32)\n",
    "    for i in range(n+1): dp[i,0] = i\n",
    "    for j in range(m+1): dp[0,j] = j\n",
    "    for i in range(1, n+1):\n",
    "        for j in range(1, m+1):\n",
    "            cost = 0 if ref[i-1]==hyp[j-1] else 1\n",
    "            dp[i,j] = min(dp[i-1,j]+1, dp[i,j-1]+1, dp[i-1,j-1]+cost)\n",
    "    return dp[n,m] / max(n,1)\n",
    "\n",
    "# ===================== 학습/평가 루프 =====================\n",
    "def evaluate(model, loader, itos, blank_idx):\n",
    "    model.eval()\n",
    "    cer_sum, cnt = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, input_lens, tgt_concat, tgt_lens, paths in loader:\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            logits = model(imgs)\n",
    "            hyps_idx = greedy_decode(logits.cpu(), input_lens, blank_idx)\n",
    "            offset = 0\n",
    "            refs = []\n",
    "            for L in tgt_lens.tolist():\n",
    "                idxs = tgt_concat[offset:offset+L].tolist()\n",
    "                refs.append(\"\".join(itos[i] for i in idxs))\n",
    "                offset += L\n",
    "            hyps = []\n",
    "            for seq in hyps_idx:\n",
    "                hyps.append(\"\".join(itos[i] for i in seq if i < len(itos)))\n",
    "            for r, h in zip(refs, hyps):\n",
    "                cer_sum += cer(r, h); cnt += 1\n",
    "    return (cer_sum / max(cnt,1)) * 100.0\n",
    "\n",
    "def main():\n",
    "    # 데이터 읽기\n",
    "    train_rows = read_csv_filter(TRAIN_CSV, TRAIN_VARIANTS)\n",
    "    valid_rows = read_csv_filter(VALID_CSV, VALID_VARIANTS)\n",
    "    print(f\"train samples: {len(train_rows)}, valid samples: {len(valid_rows)}\")\n",
    "\n",
    "    # Vocab\n",
    "    stoi, itos = build_vocab(train_rows + valid_rows)\n",
    "    blank_idx = len(itos)\n",
    "    num_classes = len(itos) + 1\n",
    "\n",
    "    # Datasets / Loaders\n",
    "    train_ds = CRNNDataset(train_rows, stoi, IMG_HEIGHT, MAX_WIDTH)\n",
    "    valid_ds = CRNNDataset(valid_rows, stoi, IMG_HEIGHT, MAX_WIDTH)\n",
    "    collate_fn = lambda b: crnn_collate(b, blank_idx)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                              num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=True)\n",
    "    valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                              num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=True)\n",
    "\n",
    "    # 모델/최적화기/손실\n",
    "    model = CRNN(num_classes=num_classes).to(DEVICE)\n",
    "    ctc_loss = nn.CTCLoss(blank=blank_idx, zero_infinity=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    best_cer = float(\"inf\")\n",
    "    no_improve = 0\n",
    "\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        model.train()\n",
    "        running_loss, n_seen = 0.0, 0\n",
    "        t0 = time()\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch:02d}\", ncols=120)\n",
    "        for step, (imgs, input_lens, tgt_concat, tgt_lens, paths) in enumerate(pbar, start=1):\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            logits = model(imgs)\n",
    "            log_probs = logits.log_softmax(dim=-1)\n",
    "            loss = ctc_loss(log_probs, tgt_concat.to(DEVICE), input_lens.to(DEVICE), tgt_lens.to(DEVICE))\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            n_seen += imgs.size(0)\n",
    "            ips = n_seen / max(time()-t0, 1e-6)\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            pbar.set_postfix(loss=f\"{running_loss/step:.4f}\", lr=f\"{lr:.2e}\", ips=f\"{ips:.1f}\")\n",
    "\n",
    "        # 검증\n",
    "        val_cer = evaluate(model, valid_loader, itos, blank_idx)\n",
    "        print(f\"[Epoch {epoch:02d}] train_loss={running_loss/len(train_loader):.4f}  valid_CER={val_cer:.2f}%\")\n",
    "\n",
    "        # Early stopping & 저장\n",
    "        if val_cer < best_cer - 1e-6:\n",
    "            best_cer = val_cer\n",
    "            no_improve = 0\n",
    "            ckpt = {\n",
    "                \"model\": model.state_dict(),\n",
    "                \"itos\": itos,\n",
    "                \"stoi\": stoi,\n",
    "                \"epoch\": epoch,\n",
    "                \"valid_cer\": best_cer,\n",
    "                \"img_height\": IMG_HEIGHT,\n",
    "                \"max_width\": MAX_WIDTH,\n",
    "            }\n",
    "            torch.save(ckpt, SAVE_PATH)\n",
    "            print(f\"  >> Saved BEST to {SAVE_PATH} (CER {best_cer:.2f}%)\")\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            print(f\"  >> no improvement ({no_improve}/{EARLY_STOP_PATIENCE})\")\n",
    "            if no_improve >= EARLY_STOP_PATIENCE:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "        # 샘플 디코드 2~3개 출력\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for imgs, input_lens, tgt_concat, tgt_lens, paths in valid_loader:\n",
    "                imgs = imgs.to(DEVICE)\n",
    "                logits = model(imgs)\n",
    "                hyps_idx = greedy_decode(logits.cpu(), input_lens, blank_idx)\n",
    "                offset = 0\n",
    "                refs = []\n",
    "                for L in tgt_lens.tolist():\n",
    "                    idxs = tgt_concat[offset:offset+L].tolist()\n",
    "                    refs.append(\"\".join(itos[i] for i in idxs))\n",
    "                    offset += L\n",
    "                for i in range(min(3, len(hyps_idx))):\n",
    "                    hyp_str = \"\".join(itos[j] for j in hyps_idx[i] if j < len(itos))\n",
    "                    print(f\"  ex{i+1}: REF='{refs[i]}' | HYP='{hyp_str}'\")\n",
    "                break\n",
    "\n",
    "    print(\"Done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch128-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
