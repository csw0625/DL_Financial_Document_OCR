{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85243652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and define utility functions\n",
    "import psutil\n",
    "import gc\n",
    "import sys\n",
    "from typing import Optional\n",
    "import os, random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import Levenshtein\n",
    "\n",
    "# device and seed 설정\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "os.environ['PYTHONHASHSEED']='42'\n",
    "random.seed(42); np.random.seed(42); torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42); torch.backends.cudnn.deterministic=True; torch.backends.cudnn.benchmark=False\n",
    "\n",
    "# 메모리 관리 함수\n",
    "def check_memory_usage(context: Optional[str] = None, threshold: float = 0.9) -> bool:\n",
    "    \"\"\"메모리 사용량 체크 함수\"\"\"\n",
    "    mem = psutil.virtual_memory()\n",
    "    used_ratio = mem.used / mem.total\n",
    "    \n",
    "    if context:\n",
    "        print(f\"Memory usage at '{context}': {used_ratio:.2%} used\")\n",
    "    else:\n",
    "        print(f\"Current memory usage: {used_ratio:.2%} used\")\n",
    "\n",
    "    return used_ratio < threshold\n",
    "\n",
    "def cleanup_memory(aggressive: bool = False):\n",
    "    \"\"\"메모리 정리 함수\"\"\"\n",
    "    if aggressive:\n",
    "        for obj in gc.get_objects():\n",
    "            if hasattr(obj, 'cache_clear'):\n",
    "                obj.cache_clear()\n",
    "    \n",
    "    collected = gc.collect()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    elif torch.backends.mps.is_available():\n",
    "        if hasattr(torch.mps, 'empty_cache'):\n",
    "            torch.mps.empty_cache()\n",
    "    \n",
    "    return collected\n",
    "\n",
    "def safe_load_images(image_paths: list, max_batch_size: int = 50):\n",
    "    \"\"\"안전한 이미지 배치 로딩\"\"\"\n",
    "    for i in range(0, len(image_paths), max_batch_size):\n",
    "        batch_paths = image_paths[i:i + max_batch_size]\n",
    "        yield batch_paths\n",
    "\n",
    "        if not check_memory_usage(f\"Image batch {i//max_batch_size + 1}\"):\n",
    "            print(\"Performing memory cleanup...\")\n",
    "            cleanup_memory()\n",
    "\n",
    "# 초기 메모리 상태 체크\n",
    "check_memory_usage(\"Initial startup\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ae5ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "train_boxes_path = \"train_extracted_boxes\"\n",
    "train_labels_path = \"train_label\"\n",
    "val_boxes_path = \"val_extracted_boxes\"\n",
    "val_labels_path = \"valid_label\"\n",
    "\n",
    "# 이미지 전처리 함수\n",
    "def load_image_as_grayscale_array(image_path):\n",
    "    \"\"\"Load image and convert to grayscale array\"\"\"\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('L')  # 흑백 변환\n",
    "        return np.array(image)\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def extract_document_id(filename):\n",
    "    \"\"\"Extract document ID from filename\"\"\"\n",
    "    parts = filename.split('_box_')\n",
    "    if len(parts) >= 2:\n",
    "        return parts[0]\n",
    "    return None\n",
    "\n",
    "def load_data(boxes_path, labels_path):\n",
    "    \"\"\"\n",
    "    Load extracted boxes and corresponding JSON labels\n",
    "    \"\"\"\n",
    "    box_files = glob.glob(os.path.join(boxes_path, \"*.png\"))\n",
    "    document_boxes = defaultdict(list)\n",
    "    \n",
    "    for box_file in box_files:\n",
    "        filename = os.path.basename(box_file)\n",
    "        doc_id = extract_document_id(filename)\n",
    "        if doc_id:\n",
    "            document_boxes[doc_id].append(box_file)\n",
    "\n",
    "    json_files = glob.glob(os.path.join(labels_path, \"*.json\"))\n",
    "    json_dict = {}\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        filename = os.path.basename(json_file)\n",
    "        doc_id = filename.replace('.json', '')\n",
    "        json_dict[doc_id] = json_file\n",
    "    \n",
    "    li_X = []\n",
    "    df_y_data = []\n",
    "    \n",
    "    valid_doc_ids = set(document_boxes.keys()) & set(json_dict.keys())\n",
    "    print(f\"Processing {len(valid_doc_ids)} documents with both boxes and labels\")\n",
    "    \n",
    "    for doc_id in tqdm(sorted(valid_doc_ids)):\n",
    "        try:\n",
    "            with open(json_dict[doc_id], 'r', encoding='utf-8') as f:\n",
    "                label_data = json.load(f)\n",
    "        \n",
    "            box_paths = sorted(document_boxes[doc_id])\n",
    "            document_images = []\n",
    "            \n",
    "            for box_path in box_paths:\n",
    "                img_array = load_image_as_grayscale_array(box_path)\n",
    "                if img_array is not None:\n",
    "                    document_images.append(img_array)\n",
    "            \n",
    "            if document_images:\n",
    "                li_X.append(document_images)\n",
    "                \n",
    "                df_y_entry = {\n",
    "                    'document_id': doc_id,\n",
    "                    'bbox': label_data.get('bbox', []),\n",
    "                    'images_metadata': label_data.get('Images', {}),\n",
    "                    'dataset_metadata': label_data.get('Dataset', {}),\n",
    "                    'annotation_metadata': label_data.get('Annotation', {}),\n",
    "                    'num_boxes': len(document_images)\n",
    "                }\n",
    "                df_y_data.append(df_y_entry)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing document {doc_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df_y = pd.DataFrame(df_y_data)\n",
    "    \n",
    "    return li_X, df_y\n",
    "\n",
    "# Load training and validation data\n",
    "li_train_X, df_train_y = load_data(train_boxes_path, train_labels_path)\n",
    "li_val_X, df_val_y = load_data(val_boxes_path, val_labels_path)\n",
    "\n",
    "li_df_bbox_train = []\n",
    "for i in range(df_train_y.shape[0]):\n",
    "    li_df_bbox_train.append(pd.DataFrame(df_train_y.iloc[i]['bbox']))\n",
    "li_df_bbox_val = []\n",
    "for i in range(df_val_y.shape[0]):\n",
    "    li_df_bbox_val.append(pd.DataFrame(df_val_y.iloc[i]['bbox']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64f7742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 정의\n",
    "def build_korean_vocab():\n",
    "    \"\"\"Build comprehensive vocabulary with Korean, English, and special tokens\"\"\"\n",
    "    korean_chars = \"가나다라마바사아자차카타파하거너더러머버서어저처커터퍼허갸냐댜랴먀뱌샤야쟈챠캬탸퍄햐\"\n",
    "    korean_chars += \"개내대래매배새애재채캐태패해걔냬댸럐먜뱨섀얘쟤챼캬탸퍄혜\"\n",
    "    korean_chars += \"고노도로모보소오조초코토포호교뇨됴료묘뵤쇼요죠쵸쿄툐표효\"\n",
    "    korean_chars += \"구누두루무부수우주추쿠투푸후규뉴듀류뮤뷰슈유쥬츄큐튜퓨휴\"\n",
    "    korean_chars += \"그느드르므브스으즈츠크트프흐긔늬디리미비시이지치키티피히\"\n",
    "    \n",
    "    korean_chars += \"각간갇갈갉갊감갑값갓강갖갗같갚갛개객간갤갬갭갯갰갱\"\n",
    "    korean_chars += \"국굳굴굵굶굼굽굿궁궁군궐궤권궷궴궵귀귄귐귑귓규균귤귀규\"\n",
    "    \n",
    "    all_chars = set()\n",
    "\n",
    "    consonants = \"ㄱㄲㄴㄷㄸㄹㄻㄼㄽㄾㄿㅀㅁㅂㅃㅄㅅㅆㅇㅈㅉㅊㅋㅌㅍㅎ\"\n",
    "    vowels = \"ㅏㅐㅑㅒㅓㅔㅕㅖㅗㅘㅙㅚㅛㅜㅝㅞㅟㅠㅡㅢㅣ\"\n",
    "    \n",
    "    all_chars.update(consonants)\n",
    "    all_chars.update(vowels)\n",
    "    all_chars.update(korean_chars)\n",
    "\n",
    "    for i in range(0xAC00, 0xD7A4):\n",
    "        all_chars.add(chr(i))\n",
    "\n",
    "    english_lower = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "    english_upper = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "    all_chars.update(english_lower)\n",
    "    all_chars.update(english_upper)\n",
    "    \n",
    "    numbers = \"0123456789\"\n",
    "    all_chars.update(numbers)\n",
    "\n",
    "    punctuation = \"().,;:!?-\"\n",
    "    all_chars.update(punctuation)\n",
    "\n",
    "    email_web_chars = \"@_./\" \n",
    "    all_chars.update(email_web_chars)\n",
    "\n",
    "    additional_symbols = \"[]{}\\\"'`~#$%^&*+=<>|\\\\~\"  \n",
    "    all_chars.update(additional_symbols)\n",
    "\n",
    "    all_chars.add(\" \")\n",
    "\n",
    "    math_currency = \"±×÷=≠≤≥∞∑∫√π°℃℉€$¥₩%\"\n",
    "    all_chars.update(math_currency)\n",
    "\n",
    "    vocab_chars = sorted(list(all_chars))\n",
    "\n",
    "    char_to_idx = {'<PAD>': 0, '<BOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
    "    for i, char in enumerate(vocab_chars):\n",
    "        char_to_idx[char] = i + 4\n",
    "    \n",
    "    idx_to_char = {v: k for k, v in char_to_idx.items()}\n",
    "    \n",
    "    return char_to_idx, idx_to_char, len(char_to_idx)\n",
    "\n",
    "# 사전 구축\n",
    "char_to_idx, idx_to_char, vocab_size = build_korean_vocab()\n",
    "\n",
    "# Swin Transformer 함수\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"Image to Patch Embedding\"\"\"\n",
    "    def __init__(self, img_size=(112, 448), patch_size=4, in_chans=1, embed_dim=96):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = [img_size[0] // patch_size, img_size[1] // patch_size]\n",
    "        self.num_patches = self.patches_resolution[0] * self.patches_resolution[1]\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    \"\"\"Window based multi-head self attention (W-MSA) module with relative position bias\"\"\"\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "        # Get pair-wise relative position bias\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing='ij'))  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        nn.init.trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = F.softmax(attn, dim=-1)\n",
    "        else:\n",
    "            attn = F.softmax(attn, dim=-1)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"Partition into non-overlapping windows\"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    \"\"\"Reverse of window partition\"\"\"\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    \"\"\"Swin Transformer Block\"\"\" \n",
    "    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n",
    "                 mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0., drop_path=0.):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        if min(self.input_resolution) <= self.window_size:\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.input_resolution)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim, window_size=(self.window_size, self.window_size), num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(mlp_hidden_dim, dim),\n",
    "            nn.Dropout(drop)\n",
    "        )\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            H, W = self.input_resolution\n",
    "            img_mask = torch.zeros((1, H, W, 1))\n",
    "            h_slices = (slice(0, -self.window_size),\n",
    "                       slice(-self.window_size, -self.shift_size),\n",
    "                       slice(-self.shift_size, None))\n",
    "            w_slices = (slice(0, -self.window_size),\n",
    "                       slice(-self.window_size, -self.shift_size),\n",
    "                       slice(-self.shift_size, None))\n",
    "            cnt = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    img_mask[:, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "\n",
    "            mask_windows = window_partition(img_mask, self.window_size)\n",
    "            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
    "            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "        else:\n",
    "            attn_mask = None\n",
    "\n",
    "        self.register_buffer(\"attn_mask\", attn_mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        \n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        # Cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        # Partition windows\n",
    "        x_windows = window_partition(shifted_x, self.window_size)\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)\n",
    "\n",
    "        # W-MSA/SW-MSA\n",
    "        attn_windows = self.attn(x_windows, mask=self.attn_mask)\n",
    "\n",
    "        # Merge windows\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)\n",
    "\n",
    "        # Reverse cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "        x = x.view(B, H * W, C)\n",
    "\n",
    "        # FFN\n",
    "        x = shortcut + x\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "class PatchMerging(nn.Module):\n",
    "    \"\"\"Patch Merging Layer\"\"\"\n",
    "    def __init__(self, input_resolution, dim):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = nn.LayerNorm(4 * dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        \n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class BasicLayer(nn.Module):\n",
    "    \"\"\"A basic Swin Transformer layer\"\"\"\n",
    "    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n",
    "                 mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., downsample=None):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.depth = depth\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n",
    "                               num_heads=num_heads, window_size=window_size,\n",
    "                               shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                               mlp_ratio=mlp_ratio,\n",
    "                               qkv_bias=qkv_bias,\n",
    "                               drop=drop, attn_drop=attn_drop,\n",
    "                               drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path)\n",
    "            for i in range(depth)])\n",
    "\n",
    "        # Patch merging layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(input_resolution, dim=dim)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "        return x\n",
    "\n",
    "class SwinTransformer(nn.Module):\n",
    "    \"\"\"Swin Transformer backbone\"\"\"\n",
    "    def __init__(self, img_size=(112, 448), patch_size=4, in_chans=1, embed_dim=96,\n",
    "                 depths=[2, 6, 2], num_heads=[6, 12, 24],\n",
    "                 window_size=7, mlp_ratio=4., qkv_bias=True,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        # Patch embedding\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        patches_resolution = self.patch_embed.patches_resolution\n",
    "        self.patches_resolution = patches_resolution\n",
    "\n",
    "        # Stochastic depth\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "\n",
    "        # Build layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n",
    "                             input_resolution=(patches_resolution[0] // (2 ** i_layer),\n",
    "                                             patches_resolution[1] // (2 ** i_layer)),\n",
    "                             depth=depths[i_layer],\n",
    "                             num_heads=num_heads[i_layer],\n",
    "                             window_size=window_size,\n",
    "                             mlp_ratio=self.mlp_ratio,\n",
    "                             qkv_bias=qkv_bias,\n",
    "                             drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                             drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
    "                             downsample=PatchMerging if (i_layer < self.num_layers - 1) else None)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.norm = nn.LayerNorm(int(embed_dim * 2 ** (self.num_layers - 1)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.norm(x)  # B L C\n",
    "        return x\n",
    "\n",
    "class SwinTransformerOCR(nn.Module):\n",
    "    \"\"\"Complete Swin Transformer OCR model with encoder-decoder architecture\"\"\"\n",
    "    def __init__(self, img_size=(112, 448), vocab_size=vocab_size, max_length=32,\n",
    "                 embed_dim=96, depths=[2, 6, 2], num_heads=[6, 12, 24]):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.max_length = max_length\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Swin Transformer encoder\n",
    "        self.swin = SwinTransformer(\n",
    "            img_size=img_size,\n",
    "            embed_dim=embed_dim,\n",
    "            depths=depths,\n",
    "            num_heads=num_heads\n",
    "        )\n",
    "        \n",
    "        final_dim = int(embed_dim * 2 ** (len(depths) - 1))\n",
    "        \n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        decoder_dim = 384\n",
    "        self.feature_proj = nn.Linear(final_dim, decoder_dim)\n",
    "        \n",
    "        # Transformer decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=decoder_dim,\n",
    "            nhead=8,\n",
    "            dim_feedforward=decoder_dim * 4,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=4)\n",
    "        \n",
    "        self.output_proj = nn.Linear(decoder_dim, vocab_size)\n",
    "        \n",
    "        self.pos_encoding = nn.Parameter(torch.randn(max_length, decoder_dim)) # Positional encoding\n",
    "        self.text_embedding = nn.Embedding(vocab_size, decoder_dim)\n",
    "        \n",
    "    def forward(self, images, target_sequences=None):\n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        features = self.swin(images)  # B, H*W, C\n",
    "        \n",
    "        global_context = self.global_pool(features.transpose(1, 2)).squeeze(-1)  # B, C\n",
    "        global_context = self.feature_proj(global_context).unsqueeze(0)  # 1, B, decoder_dim\n",
    "        \n",
    "        if self.training and target_sequences is not None:\n",
    "            # Teacher forcing\n",
    "            seq_len = target_sequences.size(1)\n",
    "            target_embeds = self.text_embedding(target_sequences)  # B, seq_len, decoder_dim\n",
    "            target_embeds += self.pos_encoding[:seq_len].unsqueeze(0)  # Add positional encoding\n",
    "            \n",
    "            causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=images.device), diagonal=1).bool()\n",
    "            \n",
    "            target_embeds = target_embeds.transpose(0, 1)  # seq_len, B, decoder_dim\n",
    "            output = self.transformer_decoder(\n",
    "                target_embeds,\n",
    "                global_context,\n",
    "                tgt_mask=causal_mask\n",
    "            )\n",
    "            \n",
    "            output = self.output_proj(output.transpose(0, 1))  # B, seq_len, vocab_size\n",
    "            return output\n",
    "        else:\n",
    "            # Autoregressive inference\n",
    "            return self.generate(global_context, max_length=self.max_length)\n",
    "    \n",
    "    def generate(self, memory, max_length=32):\n",
    "        batch_size = memory.size(1)\n",
    "        device = memory.device\n",
    "        \n",
    "        generated = torch.full((batch_size, 1), char_to_idx['<BOS>'], device=device, dtype=torch.long)\n",
    "        \n",
    "        for i in range(max_length - 1):\n",
    "            current_embeds = self.text_embedding(generated)  # B, current_len, decoder_dim\n",
    "            seq_len = current_embeds.size(1)\n",
    "            current_embeds += self.pos_encoding[:seq_len].unsqueeze(0)\n",
    "            \n",
    "            causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=device), diagonal=1).bool()\n",
    "            \n",
    "            # Decoder forward\n",
    "            current_embeds = current_embeds.transpose(0, 1)  # current_len, B, decoder_dim\n",
    "            output = self.transformer_decoder(\n",
    "                current_embeds,\n",
    "                memory,\n",
    "                tgt_mask=causal_mask\n",
    "            )\n",
    "            \n",
    "            next_token_logits = self.output_proj(output[-1])  # B, vocab_size\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)  # B, 1\n",
    "            \n",
    "            generated = torch.cat([generated, next_token], dim=1)\n",
    "            \n",
    "            if torch.all(next_token.squeeze(-1) == char_to_idx['<EOS>']):\n",
    "                break\n",
    "        \n",
    "        return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07767c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class for OCR training\n",
    "class OCRDataset(Dataset):\n",
    "    def __init__(self, li_images, li_df_bbox, transform=None, max_length=32):\n",
    "        self.li_images = li_images\n",
    "        self.li_df_bbox = li_df_bbox\n",
    "        self.transform = transform\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.data_pairs = []\n",
    "        for doc_idx, (images, df_bbox) in enumerate(zip(li_images, li_df_bbox)):\n",
    "            for img_idx, image in enumerate(images):\n",
    "                if img_idx < len(df_bbox):\n",
    "                    text = df_bbox.iloc[img_idx]['data'] if 'data' in df_bbox.columns else \"\"\n",
    "                    if isinstance(text, list) and len(text) > 0:\n",
    "                        text = text[0]  \n",
    "                    elif not isinstance(text, str):\n",
    "                        text = str(text)\n",
    "                    self.data_pairs.append((image, text, doc_idx, img_idx))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image, text, doc_idx, img_idx = self.data_pairs[idx]\n",
    "        \n",
    "        # Preprocess image\n",
    "        if isinstance(image, np.ndarray):\n",
    "            image = Image.fromarray(image.astype(np.uint8))\n",
    "\n",
    "        image = image.resize((448, 112), Image.Resampling.LANCZOS)\n",
    "        \n",
    "        image = np.array(image).astype(np.float32) / 255.0 # Normalize to [0, 1]\n",
    "        image = torch.from_numpy(image).unsqueeze(0)  # torch tensor with channel dimension\n",
    "    \n",
    "        tokens = self.text_to_tokens(text)\n",
    "\n",
    "        return image, tokens, text\n",
    "\n",
    "    def text_to_tokens(self, text):\n",
    "        \"\"\"Convert text to token indices\"\"\"\n",
    "        tokens = [char_to_idx['<BOS>']]\n",
    "        \n",
    "        for char in text[:self.max_length-2]:\n",
    "            if char in char_to_idx:\n",
    "                tokens.append(char_to_idx[char])\n",
    "            else:\n",
    "                tokens.append(char_to_idx['<UNK>'])\n",
    "        \n",
    "        tokens.append(char_to_idx['<EOS>'])\n",
    "        \n",
    "        while len(tokens) < self.max_length: \n",
    "            tokens.append(char_to_idx['<PAD>']) # padding\n",
    "        \n",
    "        return torch.tensor(tokens[:self.max_length], dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function for OCR dataset\"\"\"\n",
    "    images, tokens, texts = zip(*batch)\n",
    "    \n",
    "    images = torch.stack(images, 0)\n",
    "    tokens = torch.stack(tokens, 0)\n",
    "    \n",
    "    return images, tokens, texts\n",
    "\n",
    "\n",
    "train_dataset = OCRDataset(li_train_X, li_df_bbox_train)\n",
    "val_dataset = OCRDataset(li_val_X, li_df_bbox_val)\n",
    "\n",
    "batch_size = 8  \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Helper functions\n",
    "def tokens_to_text(tokens, remove_special=True):\n",
    "    \"\"\"Convert token indices back to text\"\"\"\n",
    "    text = \"\"\n",
    "    for token in tokens:\n",
    "        if token == char_to_idx['<PAD>']:\n",
    "            break\n",
    "        elif token == char_to_idx['<EOS>']:\n",
    "            break\n",
    "        elif remove_special and token in [char_to_idx['<BOS>'], char_to_idx['<UNK>']]:\n",
    "            continue\n",
    "        else:\n",
    "            if token in idx_to_char:\n",
    "                text += idx_to_char[token]\n",
    "    return text\n",
    "\n",
    "def calculate_accuracy(predictions, targets):\n",
    "    \"\"\"Calculate character-level accuracy\"\"\"\n",
    "    pred_texts = [tokens_to_text(pred.cpu().numpy()) for pred in predictions]\n",
    "    target_texts = [tokens_to_text(target.cpu().numpy()) for target in targets]\n",
    "    \n",
    "    correct_chars = 0\n",
    "    total_chars = 0\n",
    "    \n",
    "    for pred, target in zip(pred_texts, target_texts):\n",
    "        for p, t in zip(pred, target):\n",
    "            if p == t:\n",
    "                correct_chars += 1\n",
    "            total_chars += 1\n",
    "    \n",
    "    return correct_chars / total_chars if total_chars > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ee1651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 초기화\n",
    "model = SwinTransformerOCR(\n",
    "    img_size=(112, 448),\n",
    "    vocab_size=vocab_size,\n",
    "    max_length=32,\n",
    "    embed_dim=96,\n",
    "    depths=[2, 6, 2],\n",
    "    num_heads=[6, 12, 24]\n",
    ").to(device)\n",
    "\n",
    "# Loss function and optimizer for enhanced model\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=char_to_idx['<PAD>'])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8861e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "num_epochs = 20\n",
    "best_val_accuracy = 0\n",
    "epoch_start_time = time.time()\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    batch_count = 0\n",
    "\n",
    "    text_samples = []\n",
    "\n",
    "    # Create training progress bar\n",
    "    train_pbar = tqdm(train_loader, \n",
    "                      desc=f\"Epoch {epoch} Training\", \n",
    "                      total=len(train_loader), \n",
    "                      unit=\"batch\",\n",
    "                      leave=True,\n",
    "                      position=0,\n",
    "                      file=sys.stdout)\n",
    "    \n",
    "    for batch_idx, (images, targets, texts) in enumerate(train_pbar):\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass (teacher forcing)\n",
    "        outputs = model(images, targets[:, :-1])\n",
    "        loss = criterion(outputs.reshape(-1, vocab_size), targets[:, 1:].reshape(-1))\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        with torch.no_grad():\n",
    "            predictions = torch.argmax(outputs, dim=-1)\n",
    "            accuracy = calculate_accuracy(predictions, targets[:, 1:])\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_accuracy += accuracy\n",
    "        batch_count += 1\n",
    "        \n",
    "        if batch_count <= 10:\n",
    "            text_samples.extend(texts[:2]) \n",
    "        \n",
    "        # Update progress bar with current metrics\n",
    "        train_pbar.set_postfix({\n",
    "            'Loss': f'{loss.item():.4f}',\n",
    "            'Acc': f'{accuracy:.4f}',\n",
    "            'AvgLoss': f'{total_loss / batch_count:.4f}',\n",
    "            'AvgAcc': f'{total_accuracy / batch_count:.4f}'\n",
    "        })\n",
    "        \n",
    "        if batch_count % 10 == 0:\n",
    "            train_pbar.refresh()\n",
    "        \n",
    "        if batch_count % 500 == 0:\n",
    "            if torch.backends.mps.is_available() and hasattr(torch.mps, 'empty_cache'):\n",
    "                torch.mps.empty_cache()\n",
    "    \n",
    "    train_pbar.close()\n",
    "    \n",
    "    avg_train_loss = total_loss / batch_count\n",
    "    avg_train_acc = total_accuracy / batch_count\n",
    "\n",
    "    model.eval()\n",
    "    val_total_loss = 0\n",
    "    val_total_accuracy = 0\n",
    "    val_batch_count = 0\n",
    "    val_samples = []\n",
    "\n",
    "    # Create validation progress bar \n",
    "    val_pbar = tqdm(val_loader, \n",
    "                    desc=f\"Epoch {epoch} Validation\", \n",
    "                    total=len(val_loader), \n",
    "                    unit=\"batch\",\n",
    "                    leave=True,\n",
    "                    position=1,\n",
    "                    file=sys.stdout)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, targets, texts) in enumerate(val_pbar):\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # Forward pass with teacher forcing for loss\n",
    "            model.train()\n",
    "            outputs = model(images, targets[:, :-1])\n",
    "            model.eval()\n",
    "\n",
    "            loss = criterion(outputs.reshape(-1, vocab_size), targets[:, 1:].reshape(-1))\n",
    "\n",
    "            # Generate predictions (inference mode)\n",
    "            predictions = model(images)\n",
    "            accuracy = calculate_accuracy(predictions, targets)\n",
    "            \n",
    "            val_total_loss += loss.item()\n",
    "            val_total_accuracy += accuracy\n",
    "            val_batch_count += 1\n",
    "            \n",
    "            # Collect validation samples\n",
    "            if val_batch_count <= 5:\n",
    "                val_samples.extend(texts[:2])\n",
    "            \n",
    "            # Update progress bar with current metrics\n",
    "            val_pbar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'Acc': f'{accuracy:.4f}',\n",
    "                'AvgLoss': f'{val_total_loss / val_batch_count:.4f}',\n",
    "                'AvgAcc': f'{val_total_accuracy / val_batch_count:.4f}'\n",
    "            })\n",
    "    \n",
    "    val_pbar.close()\n",
    "    \n",
    "    avg_val_loss = val_total_loss / val_batch_count\n",
    "    avg_val_acc = val_total_accuracy / val_batch_count\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # Save best model\n",
    "    if avg_val_acc > best_val_accuracy:\n",
    "        best_val_accuracy = avg_val_acc\n",
    "        torch.save(model.state_dict(), 'swin_ocr_full_best.pth')\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save(model.state_dict(), 'swin_ocr_full_final.pth')\n",
    "\n",
    "    # Save training checkpoint\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': avg_train_loss,\n",
    "        'train_accuracy': avg_train_acc,\n",
    "        'val_loss': avg_val_loss,\n",
    "        'val_accuracy': avg_val_acc,\n",
    "        'vocab_size': vocab_size,\n",
    "        'char_to_idx': char_to_idx,\n",
    "        'idx_to_char': idx_to_char\n",
    "    }, 'swin_ocr_enhanced_full_checkpoint.pth')\n",
    "    \n",
    "    # Final results\n",
    "    total_time = time.time() - epoch_start_time\n",
    "    \n",
    "    # Show sample text types processed\n",
    "    for i, text in enumerate(text_samples[:8]):\n",
    "        # Categorize text type\n",
    "        has_korean = any('\\uAC00' <= c <= '\\uD7A3' for c in text)\n",
    "        has_english = any(c.isalpha() and ord(c) < 128 for c in text)\n",
    "        has_numbers = any(c.isdigit() for c in text)\n",
    "        has_symbols = any(c in '@._-()' for c in text)\n",
    "        \n",
    "        categories = []\n",
    "        if has_korean: categories.append(\"Korean\")\n",
    "        if has_english: categories.append(\"English\")  \n",
    "        if has_numbers: categories.append(\"Numbers\")\n",
    "        if has_symbols: categories.append(\"Symbols\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56765772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CER Calculation\n",
    "def calculate_cer(model_enhanced, val_loader, device, max_samples=None): \n",
    "    model_enhanced.eval()\n",
    "    \n",
    "    total_char_errors = 0\n",
    "    total_ref_chars = 0\n",
    "    sample_count = 0\n",
    "    detailed_results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, targets, texts) in enumerate(val_loader):\n",
    "            if max_samples and sample_count >= max_samples:\n",
    "                break\n",
    "                \n",
    "            images = images.to(device)\n",
    "            predictions = model_enhanced(images)\n",
    "\n",
    "            batch_size = images.size(0)\n",
    "            for i in range(batch_size):\n",
    "                if max_samples and sample_count >= max_samples:\n",
    "                    break\n",
    "                \n",
    "                # Convert tokens back to text\n",
    "                reference_text = tokens_to_text(targets[i].cpu().numpy())\n",
    "                predicted_text = tokens_to_text(predictions[i].cpu().numpy())\n",
    "                original_text = texts[i]\n",
    "                \n",
    "                # Calculate edit distance (Levenshtein distance)\n",
    "                edit_distance = Levenshtein.distance(predicted_text, reference_text)\n",
    "                \n",
    "                # CER calculation\n",
    "                ref_length = len(reference_text)\n",
    "                if ref_length > 0:\n",
    "                    cer_sample = edit_distance / ref_length\n",
    "                else:\n",
    "                    cer_sample = 0.0 if len(predicted_text) == 0 else 1.0\n",
    "                \n",
    "                # Accumulate totals\n",
    "                total_char_errors += edit_distance\n",
    "                total_ref_chars += ref_length\n",
    "\n",
    "                result = {\n",
    "                    'sample_id': sample_count,\n",
    "                    'original': original_text,\n",
    "                    'reference': reference_text,\n",
    "                    'predicted': predicted_text,\n",
    "                    'edit_distance': edit_distance,\n",
    "                    'ref_length': ref_length,\n",
    "                    'cer': cer_sample,\n",
    "                    'perfect_match': reference_text == predicted_text\n",
    "                } # Store\n",
    "                detailed_results.append(result)\n",
    "                \n",
    "                sample_count += 1\n",
    "                \n",
    "                if sample_count % 100 == 0:\n",
    "                    current_cer = total_char_errors / max(total_ref_chars, 1)\n",
    "                    print(f\"   Processed {sample_count} samples, Current CER: {current_cer:.4f}\")\n",
    "    \n",
    "    # Calculate overall CER\n",
    "    overall_cer = total_char_errors / max(total_ref_chars, 1)\n",
    "    \n",
    "    # Calculate additional statistics\n",
    "    perfect_matches = sum(1 for r in detailed_results if r['perfect_match'])\n",
    "    sample_cers = [r['cer'] for r in detailed_results]\n",
    "    \n",
    "    results = {\n",
    "        'overall_cer': overall_cer,\n",
    "        'total_samples': sample_count,\n",
    "        'total_char_errors': total_char_errors,\n",
    "        'total_ref_chars': total_ref_chars,\n",
    "        'perfect_matches': perfect_matches,\n",
    "        'perfect_match_rate': perfect_matches / sample_count if sample_count > 0 else 0,\n",
    "        'mean_sample_cer': np.mean(sample_cers),\n",
    "        'median_sample_cer': np.median(sample_cers),\n",
    "        'min_cer': np.min(sample_cers),\n",
    "        'max_cer': np.max(sample_cers),\n",
    "        'detailed_results': detailed_results\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Calculate CER \n",
    "cer_results = calculate_cer(model, val_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
