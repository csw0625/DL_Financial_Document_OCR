{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5246839a",
   "metadata": {},
   "source": [
    "## 예측 파이프라인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020f1580",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detect→Recognize (valid): 100%|██████████| 727/727 [03:30<00:00,  3.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detection@IoU=0.8: P=0.998  R=0.999  F1=0.999  (TP=17122, FP=28, FN=23, Pred=17150, GT=17145)\n",
      "OCR CER on matched boxes: 2.07%  (matched=17122/17145)\n",
      "Saved: C:\\Users\\USER\\DL_OCR\\val_detect_recognize.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os, json, csv, math\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import box_iou\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------- 경로/설정 --------------------\n",
    "ROOT = r\"C:\\Users\\USER\\DL_OCR\\dataset\"\n",
    "IMG_DIR = Path(ROOT) / \"valid_image\"\n",
    "LBL_DIR = Path(ROOT) / \"valid_label\"\n",
    "\n",
    "DET_CKPT = r\"C:\\Users\\USER\\DL_OCR\\char_det_frcnn_best.pth\"           # Faster R-CNN ckpt\n",
    "REC_CKPT = r\"C:\\Users\\USER\\DL_OCR\\crnn_ctc_best3.pth\"  # CRNN ckpt\n",
    "\n",
    "IOU_MATCH_THR = 0.8     # 탐지 평가/매칭 임계치\n",
    "SCORE_THR = 0.8         # 예측 박스 score 필터\n",
    "MIN_BOX_WH = 2          # 너무 작은 박스 제거\n",
    "PAD_PX = 2              # 크롭 패딩\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "OUT_CSV = str(Path(ROOT).parent / \"val_detect_recognize.csv\")\n",
    "\n",
    "# -------------------- CRNN 정의/로더 --------------------\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, num_classes:int, img_h=32, cnn_out=256, hidden=256, layers=2):\n",
    "        super().__init__()\n",
    "        self.img_h = img_h\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1,64,3,1,1), nn.ReLU(True), nn.MaxPool2d(2,2),  # stride 2\n",
    "            nn.Conv2d(64,128,3,1,1), nn.ReLU(True), nn.MaxPool2d(2,2),# stride 4\n",
    "            nn.Conv2d(128,cnn_out,3,1,1), nn.ReLU(True),\n",
    "        )\n",
    "        self.rnn = nn.LSTM(cnn_out, hidden, layers, batch_first=False, bidirectional=True)\n",
    "        self.fc  = nn.Linear(hidden*2, num_classes)\n",
    "\n",
    "    def forward(self,x):\n",
    "        f = self.cnn(x)          # (B,C,H',W'), H'=img_h/4\n",
    "        f = f.mean(2)            # (B,C,W')\n",
    "        f = f.permute(2,0,1)     # (T,B,C)\n",
    "        y,_ = self.rnn(f)        # (T,B,2H)\n",
    "        return self.fc(y)        # (T,B,num_classes)\n",
    "\n",
    "def load_crnn(rec_ckpt_path: str, device=DEVICE):\n",
    "    ckpt = torch.load(rec_ckpt_path, map_location=\"cpu\",weights_only=False)\n",
    "    itos = ckpt[\"itos\"]; stoi = ckpt[\"stoi\"]\n",
    "    num_classes = len(itos) + 1\n",
    "    img_h = ckpt.get(\"img_height\", 32)\n",
    "    model = CRNN(num_classes=num_classes, img_h=img_h)\n",
    "    model.load_state_dict(ckpt[\"model\"], strict=True)\n",
    "    model.to(device).eval()\n",
    "    meta = {\n",
    "        \"itos\": itos,\n",
    "        \"stoi\": stoi,\n",
    "        \"img_height\": img_h,\n",
    "        \"max_width\": ckpt.get(\"max_width\", 512),\n",
    "        \"blank_idx\": len(itos)\n",
    "    }\n",
    "    return model, meta\n",
    "\n",
    "@torch.no_grad()\n",
    "def greedy_decode_ctc(logits: torch.Tensor, input_lengths: torch.Tensor, blank_idx: int):\n",
    "    probs = logits.log_softmax(dim=-1)\n",
    "    pred = probs.argmax(dim=-1) # (T,B)\n",
    "    T,B = pred.shape\n",
    "    hyps = []\n",
    "    for b in range(B):\n",
    "        seq, prev = [], -1\n",
    "        Tvalid = min(int(input_lengths[b].item()), T)\n",
    "        for t in range(Tvalid):\n",
    "            p = int(pred[t,b].item())\n",
    "            if p != blank_idx and p != prev:\n",
    "                seq.append(p)\n",
    "            prev = p\n",
    "        hyps.append(seq)\n",
    "    return hyps\n",
    "\n",
    "# -------------------- Faster R-CNN 로더 --------------------\n",
    "def load_detector(det_ckpt_path: str, num_classes: int = 2, device=DEVICE):\n",
    "    m = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(\n",
    "        weights=\"DEFAULT\", weights_backbone=\"DEFAULT\"\n",
    "    )\n",
    "    in_feat = m.roi_heads.box_predictor.cls_score.in_features\n",
    "    m.roi_heads.box_predictor = FastRCNNPredictor(in_feat, num_classes)\n",
    "    ckpt = torch.load(det_ckpt_path, map_location=\"cpu\",weights_only=False)\n",
    "    m.load_state_dict(ckpt[\"model\"], strict=True)\n",
    "    m.to(device).eval()\n",
    "    return m\n",
    "\n",
    "# -------------------- 유틸 --------------------\n",
    "def cer(ref: str, hyp: str) -> float:\n",
    "    n, m = len(ref), len(hyp)\n",
    "    if n == 0: return 100.0 if m>0 else 0.0\n",
    "    dp = np.zeros((n+1, m+1), dtype=np.int32)\n",
    "    dp[:,0] = np.arange(n+1); dp[0,:] = np.arange(m+1)\n",
    "    for i in range(1,n+1):\n",
    "        for j in range(1,m+1):\n",
    "            cost = 0 if ref[i-1]==hyp[j-1] else 1\n",
    "            dp[i,j] = min(dp[i-1,j]+1, dp[i,j-1]+1, dp[i-1,j-1]+cost)\n",
    "    return 100.0 * dp[n,m] / n\n",
    "\n",
    "def poly_to_xyxy(x4: List[float], y4: List[float]) -> Tuple[int,int,int,int]:\n",
    "    x1, x2 = int(min(x4)), int(max(x4))\n",
    "    y1, y2 = int(min(y4)), int(max(y4))\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "def read_gt(json_path: Path):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        js = json.load(f)\n",
    "    boxes, texts = [], []\n",
    "    for bb in js.get(\"bbox\", []):\n",
    "        x1,y1,x2,y2 = poly_to_xyxy(bb[\"x\"], bb[\"y\"])\n",
    "        boxes.append([x1,y1,x2,y2])\n",
    "        texts.append(str(bb.get(\"data\",\"\")))\n",
    "    if len(boxes)==0:\n",
    "        return torch.zeros((0,4), dtype=torch.float32), []\n",
    "    return torch.tensor(boxes, dtype=torch.float32), texts\n",
    "\n",
    "def clamp_box(x1,y1,x2,y2,W,H,pad=PAD_PX):\n",
    "    x1 = max(0, x1 - pad); y1 = max(0, y1 - pad)\n",
    "    x2 = min(W, x2 + pad); y2 = min(H, y2 + pad)\n",
    "    if x2 <= x1: x2 = min(W, x1+1)\n",
    "    if y2 <= y1: y2 = min(H, y1+1)\n",
    "    return x1,y1,x2,y2\n",
    "\n",
    "def resize_keep_ratio_pad_gray(img_pil: Image.Image, img_h: int, max_w: int):\n",
    "    img_pil = img_pil.convert(\"L\")\n",
    "    w,h = img_pil.size\n",
    "    scale = img_h / max(h,1)\n",
    "    new_w = max(1, int(round(w*scale)))\n",
    "    new_w = min(new_w, max_w)\n",
    "    img_rs = img_pil.resize((new_w, img_h), Image.BILINEAR)\n",
    "    canvas = Image.new(\"L\", (max_w, img_h), 255)\n",
    "    canvas.paste(img_rs, (0,0))\n",
    "    arr = np.array(canvas, dtype=np.float32) / 255.0\n",
    "    t = torch.from_numpy(arr).unsqueeze(0).unsqueeze(0)  # [1,1,H,W]\n",
    "    input_len = math.ceil(new_w/4)  # MaxPool 2회 → stride=4\n",
    "    return t, input_len\n",
    "\n",
    "# -------------------- 메인 파이프라인 --------------------\n",
    "@torch.no_grad()\n",
    "def main():\n",
    "    # 모델 로드\n",
    "    det = load_detector(DET_CKPT, num_classes=2, device=DEVICE)\n",
    "    rec, meta = load_crnn(REC_CKPT, device=DEVICE)\n",
    "    itos, img_h, max_w, blank_idx = meta[\"itos\"], meta[\"img_height\"], meta[\"max_width\"], meta[\"blank_idx\"]\n",
    "\n",
    "    img_paths = sorted([p for p in IMG_DIR.iterdir() if p.suffix.lower() in (\".png\",\".jpg\",\".jpeg\",\".tif\",\".bmp\")])\n",
    "    assert len(img_paths)>0, f\"No images in {IMG_DIR}\"\n",
    "\n",
    "    # 통계\n",
    "    TP=FP=FN=0\n",
    "    cer_sum=0.0; cer_cnt=0\n",
    "    matched_boxes=0; total_gt=0; total_pred=0\n",
    "\n",
    "    with open(OUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"image_path\",\"pred_x1\",\"pred_y1\",\"pred_x2\",\"pred_y2\",\"score\",\n",
    "                    \"matched\",\"gt_text\",\"pred_text\",\"cer\"])\n",
    "\n",
    "        for img_path in tqdm(img_paths, desc=\"Detect→Recognize (valid)\"):\n",
    "            # GT 로드\n",
    "            json_path = LBL_DIR / (img_path.stem + \".json\")\n",
    "            gt_boxes, gt_texts = read_gt(json_path)\n",
    "            total_gt += len(gt_boxes)\n",
    "\n",
    "            # 이미지 로드 & 탐지\n",
    "            pil = Image.open(img_path).convert(\"RGB\")\n",
    "            W,H = pil.size\n",
    "            out = det([torchvision.transforms.functional.to_tensor(pil).to(DEVICE)])[0]\n",
    "            pred_boxes = out[\"boxes\"].detach().cpu()\n",
    "            scores     = out[\"scores\"].detach().cpu()\n",
    "\n",
    "            # 점수/크기 필터\n",
    "            keep = scores >= SCORE_THR\n",
    "            pred_boxes = pred_boxes[keep]; scores = scores[keep]\n",
    "            if len(pred_boxes) > 0 and MIN_BOX_WH > 0:\n",
    "                ww = (pred_boxes[:,2]-pred_boxes[:,0])\n",
    "                hh = (pred_boxes[:,3]-pred_boxes[:,1])\n",
    "                big = (ww>=MIN_BOX_WH) & (hh>=MIN_BOX_WH)\n",
    "                pred_boxes = pred_boxes[big]; scores = scores[big]\n",
    "            total_pred += len(pred_boxes)\n",
    "\n",
    "            # 매칭 (IoU 최댓값 기준, 1:1 할당)\n",
    "            if len(pred_boxes)==0 and len(gt_boxes)==0:\n",
    "                continue\n",
    "            if len(pred_boxes)==0:\n",
    "                FN += len(gt_boxes); continue\n",
    "            if len(gt_boxes)==0:\n",
    "                FP += len(pred_boxes)\n",
    "                # 텍스트 없음으로 CSV 기록(선택)\n",
    "                for pb, sc in zip(pred_boxes, scores):\n",
    "                    x1,y1,x2,y2 = map(int, pb.tolist())\n",
    "                    x1,y1,x2,y2 = clamp_box(x1,y1,x2,y2,W,H,PAD_PX)\n",
    "                    w.writerow([str(img_path), x1,y1,x2,y2, float(sc), 0, \"\", \"\", \"\"])\n",
    "                continue\n",
    "\n",
    "            ious = box_iou(pred_boxes, gt_boxes)  # (P,G)\n",
    "            used_g = set()\n",
    "            # 간단한 그리디 매칭: 각 pred는 최고 IoU gt와 시도\n",
    "            for p in range(len(pred_boxes)):\n",
    "                gi = int(torch.argmax(ious[p]).item())\n",
    "                iou = float(ious[p,gi].item())\n",
    "                if gi not in used_g and iou >= IOU_MATCH_THR:\n",
    "                    TP += 1; used_g.add(gi); matched = True\n",
    "                    # ---- 인식(CRNN) ----\n",
    "                    x1,y1,x2,y2 = map(int, pred_boxes[p].tolist())\n",
    "                    x1,y1,x2,y2 = clamp_box(x1,y1,x2,y2,W,H,PAD_PX)\n",
    "                    crop = pil.crop((x1,y1,x2,y2))\n",
    "                    crop_t, in_len = resize_keep_ratio_pad_gray(crop, img_h=img_h, max_w=max_w)\n",
    "                    logits = rec(crop_t.to(DEVICE))         # [T,1,C]\n",
    "                    hyp_idx = greedy_decode_ctc(logits.cpu(), torch.tensor([in_len]), blank_idx)[0]\n",
    "                    hyp = \"\".join(itos[j] for j in hyp_idx if j < len(itos))\n",
    "                    ref = gt_texts[gi]\n",
    "                    c = cer(ref, hyp)\n",
    "                    cer_sum += c; cer_cnt += 1; matched_boxes += 1\n",
    "                    w.writerow([str(img_path), x1,y1,x2,y2, float(scores[p]), 1, ref, hyp, f\"{c:.2f}\"])\n",
    "                else:\n",
    "                    # 매칭 실패 → FP\n",
    "                    FP += 1; matched = False\n",
    "                    x1,y1,x2,y2 = map(int, pred_boxes[p].tolist())\n",
    "                    x1,y1,x2,y2 = clamp_box(x1,y1,x2,y2,W,H,PAD_PX)\n",
    "                    # 매칭 실패한 것은 텍스트 비교대상이 없으므로 빈칸으로 기록\n",
    "                    w.writerow([str(img_path), x1,y1,x2,y2, float(scores[p]), 0, \"\", \"\", \"\"])\n",
    "            FN += (len(gt_boxes) - len(used_g))\n",
    "\n",
    "    P = TP / (TP+FP+1e-9)\n",
    "    R = TP / (TP+FN+1e-9)\n",
    "    F1 = 2*P*R / (P+R+1e-9)\n",
    "    avg_cer = (cer_sum / max(cer_cnt,1))\n",
    "    print(f\"\\nDetection@IoU={IOU_MATCH_THR}: P={P:.3f}  R={R:.3f}  F1={F1:.3f}  \"\n",
    "          f\"(TP={TP}, FP={FP}, FN={FN}, Pred={total_pred}, GT={total_gt})\")\n",
    "    print(f\"OCR CER on matched boxes: {avg_cer:.2f}%  (matched={matched_boxes}/{total_gt})\")\n",
    "    print(f\"Saved: {OUT_CSV}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d6e883",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46430e7d",
   "metadata": {},
   "source": [
    "## 우리 손글씨 예측시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d334fdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 경로만 정확히 맞추기 =====\n",
    "ROOT = r\"C:\\Users\\USER\\DL_OCR\\dataset\"\n",
    "TEST_DIR = Path(ROOT) / \"test_image\"             # ← 여기!  datasets/test 아님\n",
    "OUT_CSV_TEST = str(Path(ROOT).parent / \"test_detect_recognize.csv\")\n",
    "VIS_DIR = Path(ROOT).parent / \"test_vis\"\n",
    "SCORE_THR = 0.7\n",
    "VIS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323f8a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 이미지 수집을 rglob로(하위 폴더까지) + 디버그 로그 =====\n",
    "def _collect_images(folder: Path):\n",
    "    exts = {\".png\",\".jpg\",\".jpeg\",\".tif\",\".tiff\",\".bmp\",\".gif\"}\n",
    "    return sorted([p for p in folder.rglob(\"*\") if p.suffix.lower() in exts])\n",
    "\n",
    "@torch.no_grad()\n",
    "def main_test_debug():\n",
    "    print(f\"[DBG] TEST_DIR={TEST_DIR}\")\n",
    "    if not TEST_DIR.exists():\n",
    "        print(\"[ERR] TEST_DIR 없음\"); return\n",
    "    img_paths = _collect_images(TEST_DIR)\n",
    "    print(f\"[DBG] 이미지 개수: {len(img_paths)}  (예: {img_paths[0] if img_paths else '없음'})\")\n",
    "    print(f\"[DBG] 결과 CSV: {OUT_CSV_TEST}\")\n",
    "    print(f\"[DBG] VIS_DIR: {VIS_DIR}\")\n",
    "\n",
    "    if len(img_paths) == 0:\n",
    "        print(\"[ERR] 처리할 이미지가 없습니다.\"); return\n",
    "\n",
    "    det = load_detector(DET_CKPT, num_classes=2, device=DEVICE)\n",
    "    rec, meta = load_crnn(REC_CKPT, device=DEVICE)\n",
    "    itos, img_h, max_w, blank_idx = meta[\"itos\"], meta[\"img_height\"], meta[\"max_width\"], meta[\"blank_idx\"]\n",
    "    font = _load_korean_font(18)\n",
    "\n",
    "    with open(OUT_CSV_TEST, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f); w.writerow([\"image_path\",\"x1\",\"y1\",\"x2\",\"y2\",\"score\",\"pred_text\"])\n",
    "\n",
    "        for img_path in tqdm(img_paths, desc=\"Detect→Recognize (TEST)\"):\n",
    "            pil = Image.open(img_path).convert(\"RGB\"); W,H = pil.size\n",
    "            out = det([torchvision.transforms.functional.to_tensor(pil).to(DEVICE)])[0]\n",
    "            boxes = out[\"boxes\"].detach().cpu(); scores = out[\"scores\"].detach().cpu()\n",
    "\n",
    "            # 필터\n",
    "            keep = scores >= SCORE_THR\n",
    "            boxes, scores = boxes[keep], scores[keep]\n",
    "            if len(boxes)>0 and MIN_BOX_WH>0:\n",
    "                ww = boxes[:,2]-boxes[:,0]; hh = boxes[:,3]-boxes[:,1]\n",
    "                big = (ww>=MIN_BOX_WH) & (hh>=MIN_BOX_WH)\n",
    "                boxes, scores = boxes[big], scores[big]\n",
    "\n",
    "            # 그래도 0개면 최상위 1개 fallback\n",
    "            if len(boxes)==0 and out[\"boxes\"].shape[0]>0:\n",
    "                top = int(torch.argmax(out[\"scores\"]).item())\n",
    "                boxes = out[\"boxes\"][top:top+1].detach().cpu()\n",
    "                scores = out[\"scores\"][top:top+1].detach().cpu()\n",
    "\n",
    "            # 항상 시각화 파일은 저장\n",
    "            if len(boxes)==0:\n",
    "                out_path = VIS_DIR / (img_path.stem + \"_vis.png\")\n",
    "                pil.save(out_path)\n",
    "                continue\n",
    "\n",
    "            idxs = torch.argsort(scores, descending=True).tolist()\n",
    "            for i in idxs:\n",
    "                x1,y1,x2,y2 = map(int, boxes[i].tolist())\n",
    "                x1,y1,x2,y2 = clamp_box(x1,y1,x2,y2,W,H,PAD_PX)\n",
    "                crop = pil.crop((x1,y1,x2,y2))\n",
    "                crop_t, in_len = resize_keep_ratio_pad_gray(crop, img_h=img_h, max_w=max_w)\n",
    "                logits = rec(crop_t.to(DEVICE))\n",
    "                hyp_idx = greedy_decode_ctc(logits.cpu(), torch.tensor([in_len]), blank_idx)[0]\n",
    "                hyp = \"\".join(itos[j] for j in hyp_idx if j < len(itos))\n",
    "                pil = draw_box_and_text(pil, (x1,y1,x2,y2), hyp, font=font)\n",
    "                w.writerow([str(img_path), x1,y1,x2,y2, float(scores[i]), hyp])\n",
    "\n",
    "            (VIS_DIR / f\"{img_path.stem}_vis.png\").write_bytes(pil.tobytes() if hasattr(pil, \"tobytes\") else b\"\")\n",
    "            pil.save(VIS_DIR / f\"{img_path.stem}_vis.png\")   # PNG로 저장\n",
    "    print(\"[DBG] 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d2a957e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DBG] TEST_DIR=C:\\Users\\USER\\DL_OCR\\dataset\\test_image\n",
      "[DBG] 이미지 개수: 9  (예: C:\\Users\\USER\\DL_OCR\\dataset\\test_image\\권서영_250910_155950-6.png)\n",
      "[DBG] 결과 CSV: C:\\Users\\USER\\DL_OCR\\test_detect_recognize.csv\n",
      "[DBG] VIS_DIR: C:\\Users\\USER\\DL_OCR\\test_vis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detect→Recognize (TEST): 100%|██████████| 9/9 [00:02<00:00,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DBG] 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ===== 엔트리포인트: 테스트만 강제 실행 =====\n",
    "if __name__ == \"__main__\":\n",
    "    main_test_debug()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch128-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
